
# LLM Preference Classifier using Fine-Tuned DistilBERT

This project implements a 3-class classification model using a fine-tuned DistilBERT architecture to evaluate and compare outputs generated by Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini. The task is to predict which of two model-generated responses is preferred — or if they are equally good — based on human-labeled data.

## Project Overview

- **Task**: Preference classification (A wins, B wins, Tie)
- **Dataset**: [Kaggle - LLM Comparative Classification](https://www.kaggle.com/competitions/llm-classification-finetuning)
- **Model**: DistilBERT fine-tuned with Hugging Face Transformers
- **Metrics**:
  - Accuracy: 0.4607
  - Weighted F1-score: 0.4605

## Dataset

The dataset consists of over 57,000 prompt-response pairs with human preference labels.

- `prompt`: The instruction given to both models.
- `response_a` / `response_b`: Two candidate model outputs.
- `winner_model_a` / `winner_model_b`: Binary indicators of preferred output.
- `id`: Sample ID.

Three-class encoding:
- `0` → A wins
- `1` → B wins
- `2` → Tie

## Methodology

- Combined prompt + both responses into a single input string:
  ```
  Prompt: <prompt> [A]: <response_a> [B]: <response_b>
  ```
- Tokenized with DistilBERT tokenizer (`max_length=512`)
- Trained using Hugging Face `Trainer` API with the following hyperparameters:

```python
learning_rate = 1e-5
epochs = 8
batch_size = 8 (train), 16 (eval)
optimizer = AdamW
loss_fn = CrossEntropyLoss
```

- Mixed Precision (fp16) and Gradient Accumulation used for faster, efficient training.

## Results

| Class      | Precision | Recall | F1-Score |
|------------|-----------|--------|----------|
| Model A    | 0.4645    | 0.4881 | 0.4760   |
| Model B    | 0.4702    | 0.4442 | 0.4568   |
| Tie        | 0.4461    | 0.4489 | 0.4475   |

Confusion Matrix indicates overlapping predictions between A/B and ties.

## Baseline Comparison

| Model        | Accuracy |
|--------------|----------|
| DistilBERT   | **0.4607** |
| LoRA + BERT  | 0.34     |

## Key Strengths

- Lightweight model with reasonable inference speed
- Balanced performance across classes
- Modular pipeline using Hugging Face tools

## Limitations

- Semantic similarity between responses makes classification inherently difficult
- "Tie" class is ambiguous and hard to learn
- Model may not generalize well to unseen prompt styles or multi-turn dialogue

## Future Work

- Fine-tune on curated preference datasets
- Try instruction-tuned models (e.g., FLAN-T5)
- Incorporate metadata features (e.g., response length)
- Explore contrastive learning or reinforcement learning

## Authors

- Kıvılcım Naz Böke
- Mesude Gökpınar
- Dilara Efe

_CENG481 - Çankaya University | 2025_

## References

- Devlin et al., BERT (2018)
- Sanh et al., DistilBERT (2019)
- Hu et al., LoRA (2021)
- Hugging Face Transformers Documentation
- Kaggle LLM Classification Challenge (2025)
