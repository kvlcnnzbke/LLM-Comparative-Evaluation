
# LLM Preference Classifier using Fine-Tuned DistilBERT

This project implements a 3-class classification model using a fine-tuned DistilBERT architecture to evaluate and compare outputs generated by Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini. The task is to predict which of two model-generated responses is preferred â€” or if they are equally good â€” based on human-labeled data.

## ğŸš€ Project Overview

- **Task**: Preference classification (A wins, B wins, Tie)
- **Dataset**: [Kaggle - LLM Comparative Classification](https://www.kaggle.com/competitions/llm-classification-finetuning)
- **Model**: DistilBERT fine-tuned with Hugging Face Transformers
- **Metrics**:
  - Accuracy: 0.4607
  - Weighted F1-score: 0.4605

## ğŸ“Š Dataset

The dataset consists of over 57,000 prompt-response pairs with human preference labels.

- `prompt`: The instruction given to both models.
- `response_a` / `response_b`: Two candidate model outputs.
- `winner_model_a` / `winner_model_b`: Binary indicators of preferred output.
- `id`: Sample ID.

Three-class encoding:
- `0` â†’ A wins
- `1` â†’ B wins
- `2` â†’ Tie

## âš™ï¸ Methodology

- Combined prompt + both responses into a single input string:
  ```
  Prompt: <prompt> [A]: <response_a> [B]: <response_b>
  ```
- Tokenized with DistilBERT tokenizer (`max_length=512`)
- Trained using Hugging Face `Trainer` API with the following hyperparameters:

```python
learning_rate = 1e-5
epochs = 8
batch_size = 8 (train), 16 (eval)
optimizer = AdamW
loss_fn = CrossEntropyLoss
```

- Mixed Precision (fp16) and Gradient Accumulation used for faster, efficient training.

## ğŸ“ˆ Results

| Class      | Precision | Recall | F1-Score |
|------------|-----------|--------|----------|
| Model A    | 0.4645    | 0.4881 | 0.4760   |
| Model B    | 0.4702    | 0.4442 | 0.4568   |
| Tie        | 0.4461    | 0.4489 | 0.4475   |

Confusion Matrix indicates overlapping predictions between A/B and ties.

## ğŸ” Baseline Comparison

| Model        | Accuracy |
|--------------|----------|
| DistilBERT   | **0.4607** |
| LoRA + BERT  | 0.34     |

## ğŸ“Œ Key Strengths

- Lightweight model with reasonable inference speed
- Balanced performance across classes
- Modular pipeline using Hugging Face tools

## âš ï¸ Limitations

- Semantic similarity between responses makes classification inherently difficult
- "Tie" class is ambiguous and hard to learn
- Model may not generalize well to unseen prompt styles or multi-turn dialogue

## ğŸ”­ Future Work

- Fine-tune on curated preference datasets
- Try instruction-tuned models (e.g., FLAN-T5)
- Incorporate metadata features (e.g., response length)
- Explore contrastive learning or reinforcement learning

## ğŸ‘¨â€ğŸ’» Authors

- [KÄ±vÄ±lcÄ±m Naz BÃ¶ke]
- [Mesude GÃ¶kpÄ±nar]
- [Dilara Efe]

_CENG481 - Ã‡ankaya University | 2025_

## ğŸ“š References

- Devlin et al., BERT (2018)
- Sanh et al., DistilBERT (2019)
- Hu et al., LoRA (2021)
- Hugging Face Transformers Documentation
- Kaggle LLM Classification Challenge (2025)
