import pandas as pd
from datasets import Dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate

#The dataset can be found in the competition: https://www.kaggle.com/competitions/llm-classification-finetuning
train = pd.read_csv('./train.csv')
test = pd.read_csv('./test.csv')

print("Class Distribution:")
print(train[['winner_model_a', 'winner_model_b']].sum())



def label(row):
    if row['winner_model_a'] == 1:
        return 0
    elif row['winner_model_b'] == 1:
        return 1
    else:
        return 2

train['label'] = train.apply(label, axis=1)


def combine(row):
    return f"Prompt: {row['prompt']} [A]: {row['response_a']} [B]: {row['response_b']}"

train['text'] = train.apply(combine, axis=1)
test['text'] = test.apply(combine, axis=1)


dataset = Dataset.from_pandas(train[['text', 'label']])
test_dataset = Dataset.from_pandas(test[['id', 'text']])


tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize(example):
    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=512)

dataset = dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)


model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)


accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)


training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    num_train_epochs=8,
    weight_decay=0.01,
    learning_rate=1e-5,
    warmup_steps=500,
    logging_dir="./logs",
    save_total_limit=2,
    fp16=True,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    gradient_accumulation_steps=2,
    report_to="none"
)

#Validation
dataset = dataset.train_test_split(test_size=0.1)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)


trainer.train()

# Test predictions
predictions = trainer.predict(test_dataset)
preds = np.argmax(predictions.predictions, axis=1)

# Submission
submission = pd.DataFrame({
    'id': test['id'],
    'winner_model_a': (preds == 0).astype(int),
    'winner_model_b': (preds == 1).astype(int),
    'winner_model_tie': (preds == 2).astype(int),
})
submission.to_csv("bert_submission.csv", index=False)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Class Distribution Countplot
plt.figure(figsize=(10, 6))
sns.countplot(x='label', data=train, palette='viridis')
plt.title('Class Distribution - Model Winners', fontsize=14)
plt.xlabel('Winning Model', fontsize=12)
plt.ylabel('Census', fontsize=12)
plt.xticks([0, 1, 2], ['Model A', 'Model B', 'Tie'])
plt.grid(axis='y', linestyle='--', alpha=0.7)

for p in plt.gca().patches:
    plt.gca().annotate(f'{int(p.get_height())}',
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha='center', va='center',
                      xytext=(0, 5),
                      textcoords='offset points',
                      fontsize=10)

plt.savefig('class_distribution.png', bbox_inches='tight', dpi=300)
plt.close()

val_pred = trainer.predict(dataset['test'])
preds = np.argmax(val_pred.predictions, axis=1)
labels = dataset['test']['label']


accuracy = accuracy_score(labels, preds)
precision = precision_score(labels, preds, average='weighted')
recall = recall_score(labels, preds, average='weighted')
f1 = f1_score(labels, preds, average='weighted')


precision_per_class = precision_score(labels, preds, average=None)
recall_per_class = recall_score(labels, preds, average=None)
f1_per_class = f1_score(labels, preds, average=None)

#Metrics
print("\n" + "="*50)
print("Class Distribution:")
print(train['label'].value_counts().rename({0: 'Model A', 1: 'Model B', 2: 'Tie'}))

print("\n" + "="*50)
print("General Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Weighted Precision: {precision:.4f}")
print(f"Weighted Recall: {recall:.4f}")
print(f"Weighted F1-Score: {f1:.4f}")

print("\nClass-Based Metrics:")
metrics_df = pd.DataFrame({
    'Class': ['Model A', 'Model B', 'Tie'],
    'Precision': precision_per_class,
    'Recall': recall_per_class,
    'F1-Score': f1_per_class
})
print(metrics_df.to_string(index=False))

# Classification Report
print("\n" + "="*50)
print("Classification Report:")
print(classification_report(labels, preds, target_names=['Model A', 'Model B', 'Tie'], digits=4))

# Confusion matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(labels, preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Model A', 'Model B', 'Tie'],
            yticklabels=['Model A', 'Model B', 'Tie'])
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('Real Values', fontsize=12)

